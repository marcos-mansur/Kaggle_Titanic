{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49dc53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#Model_Selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from Pipeline import get_drop_categorical_features,get_drop_columns_with_null_valuse,get_colums_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7045a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nice setup for graphics cause why not: pretty blue and another pretty blue\n",
    "colors = ['#06344d', '#00b2ff']\n",
    "sns.set(palette = colors, font = 'Serif', style = 'white', \n",
    "        rc = {'axes.facecolor':'#f1f1f1', 'figure.facecolor':'#f1f1f1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3eb6c",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83dccbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dados/train.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2615096b",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16563519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass  \\\n",
       "PassengerId                     \n",
       "1                   0       3   \n",
       "2                   1       1   \n",
       "\n",
       "                                                          Name     Sex   Age  \\\n",
       "PassengerId                                                                    \n",
       "1                                      Braund, Mr. Owen Harris    male  22.0   \n",
       "2            Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
       "\n",
       "             SibSp  Parch     Ticket     Fare Cabin Embarked  \n",
       "PassengerId                                                   \n",
       "1                1      0  A/5 21171   7.2500   NaN        S  \n",
       "2                1      0   PC 17599  71.2833   C85        C  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281286b3",
   "metadata": {},
   "source": [
    "## Understanding the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f404957b",
   "metadata": {},
   "source": [
    "- `PassangerId` - numerical ID, works as an Index. Doesn't have value for the model we're going to built.\n",
    "- `Survived` - boolean var meaning (0 = did not survived, 1 = survived)\n",
    "- `Pclass` - travel class, being 1st class the most luxurious and expensive and 3rd class the most cheap\n",
    "- `Name` - passanger's names and names their relatives in parenthesis\n",
    "- `SibSp` - how many siblings the passanger had in titanic\n",
    "- `Parch` -  how many parents/childs the passanger had on board\n",
    "- `Ticket` - ticket's code, doesn't have value for the model we're going to built.\n",
    "- `Fare` - how much the ticket cost\n",
    "- `Embarked` - Port of embarkation: C = Cherbourg, Q = Queenstown, S = Southampton "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c15a5",
   "metadata": {},
   "source": [
    "## Dealing with NaN and object type columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d1da339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 1 to 891\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Survived       891 non-null    int64  \n",
      " 1   Pclass         891 non-null    int64  \n",
      " 2   Name           891 non-null    object \n",
      " 3   Sex            891 non-null    object \n",
      " 4   Age            714 non-null    float64\n",
      " 5   SibSp          891 non-null    int64  \n",
      " 6   Parch          891 non-null    int64  \n",
      " 7   Ticket         891 non-null    object \n",
      " 8   Fare           891 non-null    float64\n",
      " 9   Cabin          204 non-null    object \n",
      " 10  Embarked       889 non-null    object \n",
      " 11  Gender_binary  0 non-null      object \n",
      "dtypes: float64(2), int64(4), object(6)\n",
      "memory usage: 122.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d520cd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived     0.000000\n",
       "Pclass       0.000000\n",
       "Name         0.000000\n",
       "Sex          0.000000\n",
       "Age         19.865320\n",
       "SibSp        0.000000\n",
       "Parch        0.000000\n",
       "Ticket       0.000000\n",
       "Fare         0.000000\n",
       "Cabin       77.104377\n",
       "Embarked     0.224467\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see how much (%) of each column is missing \n",
    "faltantes_percentual = (df.isnull().sum() / len(df.iloc[:,0])*100)\n",
    "faltantes_percentual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cf33141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to change object data to categorical\n",
    "def sex_to_binary(n):\n",
    "    if n == 'male':\n",
    "        return 1\n",
    "    elif n == 'female':\n",
    "        return 0\n",
    "    \n",
    "def Pclass_onehot(df):\n",
    "    df_Pclass_enc = pd.get_dummies(df['Pclass']) \n",
    "    return df_Pclass_enc\n",
    "    \n",
    "def transform_dtype(df):\n",
    "    df['Gender_binary'] = df['Sex'].map(sex_to_binary)\n",
    "    Pclass_dummies = Pclass_onehot(df)\n",
    "    df = df.join(Pclass_dummies)\n",
    "    df.drop(columns=['Name','Ticket','Cabin','Embarked','Sex','Pclass'], inplace=True)\n",
    "    return df\n",
    "    \n",
    "def dealing_null_values(df):\n",
    "    df = df['Age'].fillna(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0378891",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = transform_dtype(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6be3479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Gender_binary</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived   Age  SibSp  Parch     Fare  Gender_binary  1  2  3\n",
       "PassengerId                                                               \n",
       "1                   0  22.0      1      0   7.2500              1  0  0  1\n",
       "2                   1  38.0      1      0  71.2833              0  1  0  0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08129963",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8219b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(\"Survived\",axis=1).copy()\n",
    "y = df.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a476a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass                                               Name  \\\n",
       "PassengerId                                                              \n",
       "1                 3                            Braund, Mr. Owen Harris   \n",
       "2                 1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "\n",
       "                Sex   Age  SibSp  Parch     Ticket     Fare Cabin Embarked  \n",
       "PassengerId                                                                 \n",
       "1              male  22.0      1      0  A/5 21171   7.2500   NaN        S  \n",
       "2            female  38.0      1      0   PC 17599  71.2833   C85        C  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d6358de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId\n",
       "1    0\n",
       "2    1\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6267ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x,y,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f6e56b",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff41594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8535cee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINALIZADO!!!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#Model_Selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from Pipeline import get_drop_categorical_features,get_drop_columns_with_null_valuse,get_colums_names\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# function to be applied in each row of the \"Sex\" column\n",
    "# and change object data to categorical (1 for male, 0 for female\"\n",
    "def sex_to_binary(n):\n",
    "    if n == 'male':\n",
    "        return 1\n",
    "    elif n == 'female':\n",
    "        return 0\n",
    "\n",
    "#transform Pclass int stype to categorical through get_dummies\n",
    "def Pclass_onehot(df):\n",
    "    df_Pclass_enc = pd.get_dummies(df['Pclass'])\n",
    "    return df_Pclass_enc\n",
    "\n",
    "# transform  all columns necessary\n",
    "def transform_dtype(df):\n",
    "    df['Gender_binary'] = df['Sex'].map(sex_to_binary)\n",
    "    Pclass_dummies = Pclass_onehot(df)\n",
    "    df = df.join(Pclass_dummies)\n",
    "    df.drop(columns=['Name' ,'Ticket' ,'Cabin' ,'Embarked' ,'Sex' ,'Pclass'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# dealing with missing numbers\n",
    "def dealing_null_values(df):\n",
    "    df = df['Age'].fillna(-1)\n",
    "    return df\n",
    "\n",
    "def saving_columns(df):\n",
    "    global colunas\n",
    "    colunas= df.columns\n",
    "    return df\n",
    "\n",
    "get_dealing_null_values = FunctionTransformer(dealing_null_values,validate=False)\n",
    "get_transform_dtype = FunctionTransformer(transform_dtype,validate=False)\n",
    "get_colums_names = FunctionTransformer(saving_columns,validate=False)\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "def drop_columns_with_null_valuse(df):\n",
    "    df.dropna(axis=1, inplace=True)\n",
    "    try:\n",
    "        df.drop(\"Fare\", axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_categorical_features(df):\n",
    "    df = df.select_dtypes(exclude=\"object\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def saving_columns(df):\n",
    "    global colunas\n",
    "    colunas = df.columns\n",
    "    return df\n",
    "\n",
    "\n",
    "get_drop_columns_with_null_valuse = FunctionTransformer(drop_columns_with_null_valuse, validate=False)\n",
    "get_drop_categorical_features = FunctionTransformer(drop_categorical_features, validate=False)\n",
    "\n",
    "df = pd.read_csv(\"Dados/train.csv\",index_col=0)\n",
    "x = df.drop(\"Survived\",axis=1).copy()\n",
    "y = df.Survived\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x,y,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "\n",
    "pipe1 = Pipeline(memory=None,\n",
    "                 steps = [\n",
    "                     (\"Feature_Selection\",get_drop_categorical_features),\n",
    "                     (\"Null_Validate\",get_drop_columns_with_null_valuse),\n",
    "                     (\"Final_Columns\",get_colums_names),\n",
    "                     (\"RandomForest\", RandomForestClassifier() )\n",
    "                 ],\n",
    "                verbose=False\n",
    "                )\n",
    "\n",
    "pipe2 = Pipeline(memory=None,\n",
    "                 steps = [\n",
    "                     (\"Feature_Selection\",get_drop_categorical_features),\n",
    "                     (\"Null_Validate\",get_drop_columns_with_null_valuse),\n",
    "                     (\"Final_Columns\",get_colums_names),\n",
    "                     (\"RandomForest\", LogisticRegression() )\n",
    "                 ],\n",
    "                verbose=False\n",
    "                )\n",
    "\n",
    "\n",
    "#Salvando Scores\n",
    "modelos_testados = {\"Modelos\":[\"RandomForestClassifier\",\"LogisticRegression\"],\n",
    "                    \"Pipeline\":[pipe1,pipe2],\n",
    "                    \"Score\":[],\n",
    "                    \"Steps\":[]\n",
    "                    }\n",
    "n = len(modelos_testados[\"Modelos\"])\n",
    "with open(\"metrics.txt\", 'w') as outfile:\n",
    "    for ref in range(n):\n",
    "        modelos_testados[\"Pipeline\"][ref].fit(x_train,y_train)\n",
    "        test_score = modelos_testados[\"Pipeline\"][ref].score(x_val,y_val)\n",
    "        nome_modelo = modelos_testados[\"Modelos\"][ref]\n",
    "        steps = modelos_testados[\"Pipeline\"][ref].named_steps.keys()\n",
    "        outfile.write(f\"{nome_modelo}- Test Score: {test_score} - Steps: {steps}\")\n",
    "        modelos_testados[\"Score\"].append(test_score)\n",
    "        lista_steps = [step for step in steps]\n",
    "        modelos_testados[\"Steps\"].append(lista_steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_modelos = pd.DataFrame({\"Model\":modelos_testados[\"Modelos\"], \"Score\":modelos_testados[\"Score\"], \"Steps\":modelos_testados[\"Steps\"]})\n",
    "df_modelos.to_markdown(\"Modelos.md\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Salvando submissão\n",
    "#Aqui devemos escolher nosso Pipe que iremos utilizar para nosso modelo\n",
    "x_test = pd.read_csv(\"Dados/test.csv\",index_col = 0)\n",
    "predict_array = pipe1.predict(x_test)\n",
    "predict_submission = pd.DataFrame({\"PassengerId\":x_test.index,\"Survived\":predict_array})\n",
    "predict_submission.to_csv(\"Predições/Predict1.csv\",index=False)\n",
    "\n",
    "\n",
    "#Plotando grafico\n",
    "importances = pipe1.named_steps['RandomForest'].feature_importances_\n",
    "colunas = ['Pclass', 'SibSp', 'Parch']\n",
    "# Colunas vai vir da var global em get_colum_names\n",
    "feature_df = pd.DataFrame(list(zip(colunas, importances)), columns = [\"feature\",\"importance\"])\n",
    "feature_df = feature_df.sort_values(by='importance', ascending=False,)\n",
    "\n",
    "\n",
    "axis_fs = 18 #fontsize\n",
    "title_fs = 22 #fontsize\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "ax = sns.barplot(x=\"importance\", y=\"feature\", data=feature_df)\n",
    "ax.set_xlabel('Importance',fontsize = axis_fs)\n",
    "ax.set_ylabel('Feature', fontsize = axis_fs)#ylabel\n",
    "ax.set_title('Random forest\\nfeature importance', fontsize = title_fs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance.png\",dpi=120)\n",
    "plt.close()\n",
    "\n",
    "print(\"FINALIZADO!!!\")\n",
    "\n",
    "\n",
    "#cml-publish feature_importance.png --md >> report.md\n",
    "#          cml-send-comment report.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a398945b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-c5c65bbdd585>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"metrics.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mref\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mmodelos_testados\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mtest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelos_testados\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mnome_modelo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelos_testados\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Modelos\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \"\"\"\n\u001b[0;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mTransformed\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \"\"\"\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkw_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, X, func, kw_args)\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_identity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkw_args\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkw_args\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documentos\\GitHub\\Kaggle_Titanic\\Pipeline.py\u001b[0m in \u001b[0;36msaving_columns\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msaving_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mcolunas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mcolunas\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5463\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5464\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5465\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5467\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#Model_Selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from Pipeline import get_drop_categorical_features,get_drop_columns_with_null_valuse,get_colums_names\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Dados/train.csv\",index_col=0)\n",
    "x = df.drop(\"Survived\",axis=1).copy()\n",
    "y = df.Survived\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x,y,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "\n",
    "pipe1 = Pipeline(memory=None,\n",
    "                 steps = [\n",
    "                     (\"Feature_Selection\",get_drop_categorical_features),\n",
    "                     (\"Null_Validate\",get_drop_columns_with_null_valuse),\n",
    "                     (\"Final_Columns\",get_colums_names),\n",
    "                     (\"RandomForest\", RandomForestClassifier() )\n",
    "                 ],\n",
    "                verbose=False\n",
    "                )\n",
    "\n",
    "pipe2 = Pipeline(memory=None,\n",
    "                 steps = [\n",
    "                     (\"Feature_Selection\",get_drop_categorical_features),\n",
    "                     (\"Null_Validate\",get_drop_columns_with_null_valuse),\n",
    "                     (\"Final_Columns\",get_colums_names),\n",
    "                     (\"RandomForest\", LogisticRegression() )\n",
    "                 ],\n",
    "                verbose=False\n",
    "                )\n",
    "\n",
    "pipe3 = Pipeline(memory=None,\n",
    "                 steps = [\n",
    "                     (\"Feature_Selection\",get_transform_dtype),\n",
    "                     (\"Null_Validate\",get_dealing_null_values),\n",
    "                     (\"Final_Columns\",get_colums_names),\n",
    "                     (\"RandomForest\", RandomForestClassifier() )\n",
    "                 ],\n",
    "                verbose=False\n",
    "                )\n",
    "\n",
    "pipe4 = Pipeline(memory=None,\n",
    "                 steps = [\n",
    "                     (\"Feature_Selection\",get_transform_dtype),\n",
    "                     (\"Null_Validate\",get_dealing_null_values),\n",
    "                     (\"Final_Columns\",get_colums_names),\n",
    "                     (\"RandomForest\", LogisticRegression() )\n",
    "                 ],\n",
    "                verbose=False\n",
    "                )\n",
    "#Salvando Scores\n",
    "modelos_testados = {\"Modelos\":[\"RandomForestClassifier\",\"LogisticRegression\",\"RandomForestClassifier_v2\",\"LogisticRegression\"],\n",
    "                    \"Pipeline\":[pipe1,pipe2,pipe3, pipe4],\n",
    "                    \"Score\":[],\n",
    "                    \"Steps\":[]\n",
    "                    }\n",
    "n = len(modelos_testados[\"Modelos\"])\n",
    "with open(\"metrics.txt\", 'w') as outfile:\n",
    "    for ref in range(n):\n",
    "        modelos_testados[\"Pipeline\"][ref].fit(x_train,y_train)\n",
    "        test_score = modelos_testados[\"Pipeline\"][ref].score(x_val,y_val)\n",
    "        nome_modelo = modelos_testados[\"Modelos\"][ref]\n",
    "        steps = modelos_testados[\"Pipeline\"][ref].named_steps.keys()\n",
    "        outfile.write(f\"{nome_modelo}- Test Score: {test_score} - Steps: {steps}\")\n",
    "        modelos_testados[\"Score\"].append(test_score)\n",
    "        lista_steps = [step for step in steps]\n",
    "        modelos_testados[\"Steps\"].append(lista_steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_modelos = pd.DataFrame({\"Model\":modelos_testados[\"Modelos\"], \"Score\":modelos_testados[\"Score\"], \"Steps\":modelos_testados[\"Steps\"]})\n",
    "df_modelos.to_markdown(\"Modelos.md\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Salvando submissão\n",
    "#Aqui devemos escolher nosso Pipe que iremos utilizar para nosso modelo\n",
    "x_test = pd.read_csv(\"Dados/test.csv\",index_col = 0)\n",
    "predict_array = pipe1.predict(x_test)\n",
    "predict_submission = pd.DataFrame({\"PassengerId\":x_test.index,\"Survived\":predict_array})\n",
    "predict_submission.to_csv(\"Predições/Predict1.csv\",index=False)\n",
    "\n",
    "\n",
    "#Plotando grafico\n",
    "importances = pipe1.named_steps['RandomForest'].feature_importances_\n",
    "colunas = ['Pclass', 'SibSp', 'Parch']\n",
    "# Colunas vai vir da var global em get_colum_names\n",
    "feature_df = pd.DataFrame(list(zip(colunas, importances)), columns = [\"feature\",\"importance\"])\n",
    "feature_df = feature_df.sort_values(by='importance', ascending=False,)\n",
    "\n",
    "\n",
    "axis_fs = 18 #fontsize\n",
    "title_fs = 22 #fontsize\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "ax = sns.barplot(x=\"importance\", y=\"feature\", data=feature_df)\n",
    "ax.set_xlabel('Importance',fontsize = axis_fs)\n",
    "ax.set_ylabel('Feature', fontsize = axis_fs)#ylabel\n",
    "ax.set_title('Random forest\\nfeature importance', fontsize = title_fs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance.png\",dpi=120)\n",
    "plt.close()\n",
    "\n",
    "print(\"FINALIZADO!!!\")\n",
    "\n",
    "\n",
    "#cml-publish feature_importance.png --md >> report.md\n",
    "#          cml-send-comment report.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "964fc09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature_Selection\",get_transform_dtype),\n",
    "#                     (\"Null_Validate\",get_dealing_null_values),\n",
    "##                     (\"Final_Columns\",get_colums_names),\n",
    " #                    (\"RandomForest\", RandomForestClassifier()\n",
    " \n",
    "\n",
    "df2 = dealing_null_values(df)\n",
    "#get_dealing_null_values = FunctionTransformer(dealing_null_values,validate=False)\n",
    "#get_transform_dtype = FunctionTransformer(transform_dtype,validate=False)\n",
    "#get_colums_names = FunctionTransformer(saving_columns,validate=False)\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "258293d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId\n",
       "1    22.0\n",
       "2    38.0\n",
       "3    26.0\n",
       "4    35.0\n",
       "5    35.0\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0aec23d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# function to be applied in each row of the \"Sex\" column\n",
    "# and change object data to categorical (1 for male, 0 for female\"\n",
    "def sex_to_binary(n):\n",
    "    if n == 'male':\n",
    "        return 1\n",
    "    elif n == 'female':\n",
    "        return 0\n",
    "\n",
    "#transform Pclass int stype to categorical through get_dummies\n",
    "def Pclass_onehot(df):\n",
    "    df_Pclass_enc = pd.get_dummies(df['Pclass'])\n",
    "    return df_Pclass_enc\n",
    "\n",
    "# transform  all columns necessary\n",
    "def transform_dtype(df):\n",
    "    df['Gender_binary'] = df['Sex'].map(sex_to_binary)\n",
    "    Pclass_dummies = Pclass_onehot(df)\n",
    "    df = df.join(Pclass_dummies)\n",
    "    df.drop(['Name' ,'Ticket' ,'Cabin' ,'Embarked' ,'Sex' ,'Pclass'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# dealing with missing numbers\n",
    "def dealing_null_values(df):\n",
    "    df['Age'] = df['Age'].fillna(-1)\n",
    "    return df\n",
    "\n",
    "def saving_columns(df):\n",
    "    global colunas\n",
    "    colunas= df.columns\n",
    "    return df\n",
    "\n",
    "get_dealing_null_values = FunctionTransformer(dealing_null_values,validate=False)\n",
    "get_transform_dtype = FunctionTransformer(transform_dtype,validate=False)\n",
    "get_colums_names = FunctionTransformer(saving_columns,validate=False)\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "def drop_columns_with_null_valuse(df):\n",
    "    df.dropna(axis=1, inplace=True)\n",
    "    try:\n",
    "        df.drop(\"Fare\", axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_categorical_features(df):\n",
    "    df = df.select_dtypes(exclude=\"object\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def saving_columns(df):\n",
    "    global colunas\n",
    "    colunas = df.columns\n",
    "    return df\n",
    "\n",
    "\n",
    "get_drop_columns_with_null_valuse = FunctionTransformer(drop_columns_with_null_valuse, validate=False)\n",
    "get_drop_categorical_features = FunctionTransformer(drop_categorical_features, validate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81c38d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINALIZADO!!!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#Model_Selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from Pipeline import get_drop_categorical_features,get_drop_columns_with_null_valuse,get_colums_names\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Dados/train.csv\",index_col=0)\n",
    "x = df.drop(\"Survived\",axis=1).copy()\n",
    "y = df.Survived\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x,y,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "\n",
    "pipe1 = Pipeline(memory=None,\n",
    "                 steps = [\n",
    "                     (\"Feature_Selection\",get_drop_categorical_features),\n",
    "                     (\"Null_Validate\",get_drop_columns_with_null_valuse),\n",
    "                     (\"Final_Columns\",get_colums_names),\n",
    "                     (\"RandomForest\", RandomForestClassifier() )\n",
    "                 ],\n",
    "                verbose=False\n",
    "                )\n",
    "\n",
    "pipe2 = Pipeline(memory=None,\n",
    "                 steps = [\n",
    "                     (\"Feature_Selection\",get_drop_categorical_features),\n",
    "                     (\"Null_Validate\",get_drop_columns_with_null_valuse),\n",
    "                     (\"Final_Columns\",get_colums_names),\n",
    "                     (\"RandomForest\", LogisticRegression() )\n",
    "                 ],\n",
    "                verbose=False\n",
    "                )\n",
    "\n",
    "pipe3 = Pipeline(memory=None,\n",
    "                 steps = [\n",
    "                     (\"Feature_Selection\",get_transform_dtype),\n",
    "                     (\"Null_Validate\",get_dealing_null_values),\n",
    "                     (\"Final_Columns\",get_colums_names),\n",
    "                     (\"RandomForest\", RandomForestClassifier() )\n",
    "                 ],\n",
    "                verbose=False\n",
    "                )\n",
    "\n",
    "pipe4 = Pipeline(memory=None,\n",
    "                 steps = [\n",
    "                     (\"Feature_Selection\",get_transform_dtype),\n",
    "                     (\"Null_Validate\",get_dealing_null_values),\n",
    "                     (\"Final_Columns\",get_colums_names),\n",
    "                     (\"RandomForest\", LogisticRegression() )\n",
    "                 ],\n",
    "                verbose=False\n",
    "                )\n",
    "#Salvando Scores\n",
    "modelos_testados = {\"Modelos\":[\"RandomForestClassifier\",\"LogisticRegression\",\"RandomForestClassifier_v2\",\"LogisticRegression\"],\n",
    "                    \"Pipeline\":[pipe1,pipe2,pipe3, pipe4],\n",
    "                    \"Score\":[],\n",
    "                    \"Steps\":[]\n",
    "                    }\n",
    "n = len(modelos_testados[\"Modelos\"])\n",
    "with open(\"metrics.txt\", 'w') as outfile:\n",
    "    for ref in range(n):\n",
    "        modelos_testados[\"Pipeline\"][ref].fit(x_train,y_train)\n",
    "        test_score = modelos_testados[\"Pipeline\"][ref].score(x_val,y_val)\n",
    "        nome_modelo = modelos_testados[\"Modelos\"][ref]\n",
    "        steps = modelos_testados[\"Pipeline\"][ref].named_steps.keys()\n",
    "        outfile.write(f\"{nome_modelo}- Test Score: {test_score} - Steps: {steps}\")\n",
    "        modelos_testados[\"Score\"].append(test_score)\n",
    "        lista_steps = [step for step in steps]\n",
    "        modelos_testados[\"Steps\"].append(lista_steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_modelos = pd.DataFrame({\"Model\":modelos_testados[\"Modelos\"], \"Score\":modelos_testados[\"Score\"], \"Steps\":modelos_testados[\"Steps\"]})\n",
    "df_modelos.to_markdown(\"Modelos.md\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Salvando submissão\n",
    "#Aqui devemos escolher nosso Pipe que iremos utilizar para nosso modelo\n",
    "x_test = pd.read_csv(\"Dados/test.csv\",index_col = 0)\n",
    "predict_array = pipe1.predict(x_test)\n",
    "predict_submission = pd.DataFrame({\"PassengerId\":x_test.index,\"Survived\":predict_array})\n",
    "predict_submission.to_csv(\"Predições/Predict1.csv\",index=False)\n",
    "\n",
    "\n",
    "#Plotando grafico\n",
    "importances = pipe1.named_steps['RandomForest'].feature_importances_\n",
    "colunas = ['Pclass', 'SibSp', 'Parch']\n",
    "# Colunas vai vir da var global em get_colum_names\n",
    "feature_df = pd.DataFrame(list(zip(colunas, importances)), columns = [\"feature\",\"importance\"])\n",
    "feature_df = feature_df.sort_values(by='importance', ascending=False,)\n",
    "\n",
    "\n",
    "axis_fs = 18 #fontsize\n",
    "title_fs = 22 #fontsize\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "ax = sns.barplot(x=\"importance\", y=\"feature\", data=feature_df)\n",
    "ax.set_xlabel('Importance',fontsize = axis_fs)\n",
    "ax.set_ylabel('Feature', fontsize = axis_fs)#ylabel\n",
    "ax.set_title('Random forest\\nfeature importance', fontsize = title_fs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance.png\",dpi=120)\n",
    "plt.close()\n",
    "\n",
    "print(\"FINALIZADO!!!\")\n",
    "\n",
    "\n",
    "#cml-publish feature_importance.png --md >> report.md\n",
    "#          cml-send-comment report.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23887f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
